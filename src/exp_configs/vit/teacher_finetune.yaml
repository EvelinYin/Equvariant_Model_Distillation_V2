# =========================
# Data configuration
# =========================
data:
  dataset_name: cifar100
  data_dir: ./datasets
  imagnet_normalization: true
  batch_size: 32
  num_workers: 16
  num_classes: 100




# =========================
# Teacher model configuration
# =========================
teacher_model:
  model_structure: pretrained_ViT

  pretrained_vit_config:
    model_name: google/vit-base-patch16-224
    num_classes: 100


# =========================
# Teacher checkpoint
# =========================
teacher_train:
  epochs: 300
  learning_rate: 3e-4
  teacher_ckpt_path: ""
  # teacher_ckpt_path: "/home/yin178/Equvariant_Model_Distillation_V2/outputs/CIFAR100/pretrained_ViT/teacher/google/vit-base-patch16-224/best_fixed.ckpt"





# =========================
# Logging configuration
# =========================
logging:
  project_name: equvariant-distillation
  entity: null
  log_frequency: 50
  outputs_dir: ./outputs
  wandb_name: teacher_ViT_distillation
  wandb_mode: online


# =========================
# Global configuration
# =========================
device: cuda
seed: 42
precision: 16-mixed # "32" or "16-mixed"
